# -*- coding: utf-8 -*-
"""moneycontrol.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/manishrawat2022/ReStock/blob/main/moneycontrol.ipynb

## Moneycontrol.com Scraper Notebook

#### Install required dependencies
"""

"""#### Import the required libraries"""

import requests
import requests_html
from requests_html import HTMLSession
from bs4 import BeautifulSoup
import datetime
import pymongo
from pymongo import MongoClient

"""#### Function to compute links under a url"""

def compute_article_links(url):
  session = HTMLSession()
  r = session.get(url)

  element = r.html.find('ul#cagetory', first=True)
  return element.absolute_links

"""#### Function to scrape article data"""

def get_article_data(url):
    page = str(requests.get(url, timeout=10).content.decode("utf-8", "ignore"))
    soup = BeautifulSoup(page, "html.parser")
    article = {}

    try:
        article["title"] = soup.find(
            attrs={"class": "article_title"}).string.strip()
    except:
        return None;
      
    try:
        article["desc"] = soup.find(
            attrs={"class": "article_desc"}).string.strip()
    except:
        pass

    try:
        content = soup.select(".content_wrapper > p")
        article["content"] = " ".join(
            [c.string for c in content if c.string])
    except:
        pass
    
    try:
        author_element = soup.select_one(".content_block span")
        article["author"] = author_element.string
    except:
        pass
        
    try:
        time_date_element = soup.find(attrs={"class": "article_schedule"})
        time_date_string = ""
        for element in time_date_element.contents:
            if element and element.string.strip():
                time_date_string += element.string.strip()
        article["timestamp"] = datetime.datetime.strptime(time_date_string, "%B %d, %Y/ %I:%M %p %Z")
    except Exception as e:
        #print(e)
        try:
            tags_last_line = soup.select_one(".tags_last_line")
            time_date_string = tags_last_line.string.upper()
            article["timestamp"] = datetime.datetime.strptime(time_date_string, "FIRST PUBLISHED: %b %d, %Y %I:%M %p")
        except Exception as e:
            #print(e)
            return None

    return article


def sync_news():
    """#### Connect to MongoDB"""

    client = MongoClient('mongodb+srv://random:Random@stock.mbex3cy.mongodb.net/?retryWrites=true&w=majority')
    db = client["Stocks"]
    collection = db["moneycontrol"]
    bufferColl = db["buffer"]

    bufferColl.delete_many({})

    base_urls = {
        "business": ("https://www.moneycontrol.com/news/business", 30),
        "companies": ("https://www.moneycontrol.com/news/tags/companies.html", 30),
        "economy": ("https://www.moneycontrol.com/news/business/economy", 30),
        "personal-finance": ("https://www.moneycontrol.com/news/business/personal-finance", 30),
        "stocks": ("https://www.moneycontrol.com/news/business/stocks", 30)
        #"tech-analysis": ("https://www.moneycontrol.com/news/tags/technical-analysis.html", 30)
    }
    tsMapping = {}
    for category, url in base_urls.items():
        iterator = collection.find({"source":category}).sort("timestamp", pymongo.DESCENDING).limit(1)
        tsDoc = next(iterator, None)
        if tsDoc != None :
            tsMapping[category] = tsDoc["timestamp"]

    print(tsMapping)

    for source, url_desc in base_urls.items():
        print(f"Entering : {source}")
        base_url = url_desc[0]
        page_count = url_desc[1]
        
        finished = False
        lastTs = tsMapping.get(source)

        for i in range(1, page_count + 1):
            print(f"Processing page : {i}")
            if i==1:
                url = base_url
            else:
                url = base_url + "/page-" + str(i)+"/";
            links = compute_article_links(url)
            articles = []
            for link in links:
                try:
                    #print(f"Processing link : {link}")
                    article = get_article_data(link)
                    if article == None:
                        continue
                    article["source"] = source
                    article["link"] = link
                    
                    if lastTs == None or article["timestamp"] > lastTs:
                        articles.append(article)
                    else:
                        finished = True
                    break
                except Exception as e:
                    print(f"Exception while processing url {link}")

            if len(articles) > 0:
                print(f"Inserting {len(articles)} documents")
                bufferColl.insert_many(articles, ordered = False)
            
            if finished:
                break

    for doc in bufferColl.find({}).sort("timestamp",pymongo.DESCENDING):
        collection.insert_one(doc)

    bufferColl.delete_many({})